{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms","metadata":{"id":"kZkQ_k4AzDn_","execution":{"iopub.status.busy":"2024-04-01T13:02:19.500154Z","iopub.execute_input":"2024-04-01T13:02:19.501054Z","iopub.status.idle":"2024-04-01T13:02:19.507566Z","shell.execute_reply.started":"2024-04-01T13:02:19.501017Z","shell.execute_reply":"2024-04-01T13:02:19.506636Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"train_transforms = transforms.Compose([\n\n\n        \n   \n transforms.RandomRotation(8),\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])\n\ntest_transforms = transforms.Compose([\n\n                                       transforms.ToTensor(),\n                                       transforms.Normalize((0.1307,), (0.3081,))\n                                       ])\n","metadata":{"id":"qoTJKoUG2Ztp","execution":{"iopub.status.busy":"2024-04-01T13:02:19.509418Z","iopub.execute_input":"2024-04-01T13:02:19.509715Z","iopub.status.idle":"2024-04-01T13:02:19.515691Z","shell.execute_reply.started":"2024-04-01T13:02:19.509692Z","shell.execute_reply":"2024-04-01T13:02:19.514788Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"train = datasets.MNIST('./data', train=True, download=True, transform=train_transforms)\ntest = datasets.MNIST('./data', train=False, download=True, transform=test_transforms)","metadata":{"id":"r6LiZiuz2bFr","execution":{"iopub.status.busy":"2024-04-01T13:02:19.516974Z","iopub.execute_input":"2024-04-01T13:02:19.517339Z","iopub.status.idle":"2024-04-01T13:02:19.603190Z","shell.execute_reply.started":"2024-04-01T13:02:19.517306Z","shell.execute_reply":"2024-04-01T13:02:19.602384Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"SEED = 1\n\n# CUDA?\ncuda = torch.cuda.is_available()\nprint(\"CUDA Available?\", cuda)\n\n# For reproducibility\ntorch.manual_seed(SEED)\n\nif cuda:\n    torch.cuda.manual_seed(SEED)\n\n# dataloader arguments - something you'll fetch these from cmdprmt\ndataloader_args = dict(shuffle=True, batch_size=128, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)\n\n# train dataloader\ntrain_loader = torch.utils.data.DataLoader(train, **dataloader_args)\n\n# test dataloader\ntest_loader = torch.utils.data.DataLoader(test, **dataloader_args)","metadata":{"id":"N_jXwFX-2dgR","execution":{"iopub.status.busy":"2024-04-01T13:02:19.605431Z","iopub.execute_input":"2024-04-01T13:02:19.605872Z","iopub.status.idle":"2024-04-01T13:02:19.614134Z","shell.execute_reply.started":"2024-04-01T13:02:19.605837Z","shell.execute_reply":"2024-04-01T13:02:19.613220Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"CUDA Available? True\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch.nn.functional as F\ndropout_value = 0.1\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        # Input Block\n        self.convblock1 = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=14, kernel_size=(3, 3), padding=0, bias=False),\n            nn.ReLU(),\n            nn.BatchNorm2d(14),\n            nn.Dropout(dropout_value)\n        ) # output_size = 26\n\n        # CONVOLUTION BLOCK 1\n        self.convblock2 = nn.Sequential(\n            nn.Conv2d(in_channels=14, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n            nn.ReLU(),\n            nn.BatchNorm2d(16),\n            nn.Dropout(dropout_value)\n        ) # output_size = 24\n\n        # TRANSITION BLOCK 1\n        self.convblock3 = nn.Sequential(\n            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(1, 1), padding=0, bias=False),\n        ) # output_size = 24\n        self.pool1 = nn.MaxPool2d(2, 2) # output_size = 12\n\n        # CONVOLUTION BLOCK 2\n        self.convblock4 = nn.Sequential(\n            nn.Conv2d(in_channels=16, out_channels=12, kernel_size=(3, 3), padding=0, bias=False),\n            nn.ReLU(),            \n            nn.BatchNorm2d(12),\n            nn.Dropout(dropout_value)\n        ) # output_size = 10\n        self.convblock5 = nn.Sequential(\n            nn.Conv2d(in_channels=12, out_channels=12, kernel_size=(3, 3), padding=0, bias=False),\n            nn.ReLU(),            \n            nn.BatchNorm2d(12),\n            nn.Dropout(dropout_value)\n        ) # output_size = 8\n        self.convblock6 = nn.Sequential(\n            nn.Conv2d(in_channels=12, out_channels=12, kernel_size=(3, 3), padding=0, bias=False),\n            nn.ReLU(),            \n            nn.BatchNorm2d(12),\n            nn.Dropout(dropout_value)\n        ) # output_size = 6\n        self.convblock7 = nn.Sequential(\n            nn.Conv2d(in_channels=12, out_channels=12, kernel_size=(3, 3), padding=1, bias=False),\n            nn.ReLU(),            \n            nn.BatchNorm2d(12),\n            nn.Dropout(dropout_value)\n        ) # output_size = 6\n        \n        # OUTPUT BLOCK\n        self.gap = nn.Sequential(\n            nn.AvgPool2d(kernel_size=6)\n        ) # output_size = 1\n\n        self.convblock8 = nn.Sequential(\n            nn.Conv2d(in_channels=12, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),\n            # nn.BatchNorm2d(10),\n            # nn.ReLU(),\n            # nn.Dropout(dropout_value)\n        ) \n\n\n        self.dropout = nn.Dropout(dropout_value)\n\n    def forward(self, x):\n        x = self.convblock1(x)\n        x = self.convblock2(x)\n        x = self.convblock3(x)\n        x = self.pool1(x)\n        x = self.convblock4(x)\n        x = self.convblock5(x)\n        x = self.convblock6(x)\n        x = self.convblock7(x)\n        x = self.gap(x)        \n        x = self.convblock8(x)\n\n        x = x.view(-1, 10)\n        return F.log_softmax(x, dim=-1)","metadata":{"id":"xbvxYx_C2fEk","execution":{"iopub.status.busy":"2024-04-01T13:08:47.891304Z","iopub.execute_input":"2024-04-01T13:08:47.891698Z","iopub.status.idle":"2024-04-01T13:08:47.909319Z","shell.execute_reply.started":"2024-04-01T13:08:47.891670Z","shell.execute_reply":"2024-04-01T13:08:47.908324Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"!pip install torchsummary\nfrom torchsummary import summary\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\")\nprint(device)\nmodel = Net().to(device)\nsummary(model, input_size=(1, 28, 28))","metadata":{"id":"CsccDy0s2hMy","execution":{"iopub.status.busy":"2024-04-01T13:08:48.756102Z","iopub.execute_input":"2024-04-01T13:08:48.756488Z","iopub.status.idle":"2024-04-01T13:09:00.790595Z","shell.execute_reply.started":"2024-04-01T13:08:48.756456Z","shell.execute_reply":"2024-04-01T13:09:00.789254Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchsummary in /opt/conda/lib/python3.10/site-packages (1.5.1)\ncuda\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [-1, 14, 26, 26]             126\n              ReLU-2           [-1, 14, 26, 26]               0\n       BatchNorm2d-3           [-1, 14, 26, 26]              28\n           Dropout-4           [-1, 14, 26, 26]               0\n            Conv2d-5           [-1, 16, 24, 24]           2,016\n              ReLU-6           [-1, 16, 24, 24]               0\n       BatchNorm2d-7           [-1, 16, 24, 24]              32\n           Dropout-8           [-1, 16, 24, 24]               0\n            Conv2d-9           [-1, 16, 24, 24]             256\n        MaxPool2d-10           [-1, 16, 12, 12]               0\n           Conv2d-11           [-1, 12, 10, 10]           1,728\n             ReLU-12           [-1, 12, 10, 10]               0\n      BatchNorm2d-13           [-1, 12, 10, 10]              24\n          Dropout-14           [-1, 12, 10, 10]               0\n           Conv2d-15             [-1, 12, 8, 8]           1,296\n             ReLU-16             [-1, 12, 8, 8]               0\n      BatchNorm2d-17             [-1, 12, 8, 8]              24\n          Dropout-18             [-1, 12, 8, 8]               0\n           Conv2d-19             [-1, 12, 6, 6]           1,296\n             ReLU-20             [-1, 12, 6, 6]               0\n      BatchNorm2d-21             [-1, 12, 6, 6]              24\n          Dropout-22             [-1, 12, 6, 6]               0\n           Conv2d-23             [-1, 12, 6, 6]           1,296\n             ReLU-24             [-1, 12, 6, 6]               0\n      BatchNorm2d-25             [-1, 12, 6, 6]              24\n          Dropout-26             [-1, 12, 6, 6]               0\n        AvgPool2d-27             [-1, 12, 1, 1]               0\n           Conv2d-28             [-1, 10, 1, 1]             120\n================================================================\nTotal params: 8,290\nTrainable params: 8,290\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.74\nParams size (MB): 0.03\nEstimated Total Size (MB): 0.78\n----------------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"code","source":"from tqdm import tqdm\n\ntrain_losses = []\ntest_losses = []\ntrain_acc = []\ntest_acc = []\n\ndef train(model, device, train_loader, optimizer, epoch):\n  model.train()\n  pbar = tqdm(train_loader)\n  correct = 0\n  processed = 0\n  for batch_idx, (data, target) in enumerate(pbar):\n    # get samples\n    data, target = data.to(device), target.to(device)\n\n    # Init\n    optimizer.zero_grad()\n    # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes.\n    # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.\n\n    # Predict\n    y_pred = model(data)\n\n    # Calculate loss\n    loss = F.nll_loss(y_pred, target)\n    train_losses.append(loss)\n\n    # Backpropagation\n    loss.backward()\n    optimizer.step()\n\n    # Update pbar-tqdm\n\n    pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n    correct += pred.eq(target.view_as(pred)).sum().item()\n    processed += len(data)\n\n    pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n    train_acc.append(100*correct/processed)\n\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n    test_losses.append(test_loss)\n\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\n    test_acc.append(100. * correct / len(test_loader.dataset))","metadata":{"id":"i4FE8l622iqT","execution":{"iopub.status.busy":"2024-04-01T13:09:00.792801Z","iopub.execute_input":"2024-04-01T13:09:00.793154Z","iopub.status.idle":"2024-04-01T13:09:01.095094Z","shell.execute_reply.started":"2024-04-01T13:09:00.793124Z","shell.execute_reply":"2024-04-01T13:09:01.094188Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"from torch.optim.lr_scheduler import StepLR\n\nmodel =  Net().to(device)\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\nscheduler = StepLR(optimizer, step_size=6, gamma=0.1)\n\n\nEPOCHS = 15\nfor epoch in range(EPOCHS):\n    print(\"EPOCH:\", epoch)\n    train(model, device, train_loader, optimizer, epoch)\n    scheduler.step()\n    test(model, device, test_loader)","metadata":{"id":"p845IM0Y2kOU","execution":{"iopub.status.busy":"2024-04-01T13:09:06.609920Z","iopub.execute_input":"2024-04-01T13:09:06.610271Z","iopub.status.idle":"2024-04-01T13:11:52.548020Z","shell.execute_reply.started":"2024-04-01T13:09:06.610246Z","shell.execute_reply":"2024-04-01T13:11:52.546786Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"EPOCH: 0\n","output_type":"stream"},{"name":"stderr","text":"Loss=0.19550739228725433 Batch_id=468 Accuracy=83.36: 100%|██████████| 469/469 [00:09<00:00, 49.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nTest set: Average loss: 0.0881, Accuracy: 9752/10000 (97.52%)\n\nEPOCH: 1\n","output_type":"stream"},{"name":"stderr","text":"Loss=0.06901193410158157 Batch_id=468 Accuracy=96.86: 100%|██████████| 469/469 [00:09<00:00, 48.90it/s] \n","output_type":"stream"},{"name":"stdout","text":"\nTest set: Average loss: 0.0539, Accuracy: 9833/10000 (98.33%)\n\nEPOCH: 2\n","output_type":"stream"},{"name":"stderr","text":"Loss=0.07137814909219742 Batch_id=468 Accuracy=97.59: 100%|██████████| 469/469 [00:09<00:00, 50.17it/s] \n","output_type":"stream"},{"name":"stdout","text":"\nTest set: Average loss: 0.0455, Accuracy: 9852/10000 (98.52%)\n\nEPOCH: 3\n","output_type":"stream"},{"name":"stderr","text":"Loss=0.026898937299847603 Batch_id=468 Accuracy=97.92: 100%|██████████| 469/469 [00:09<00:00, 50.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nTest set: Average loss: 0.0366, Accuracy: 9885/10000 (98.85%)\n\nEPOCH: 4\n","output_type":"stream"},{"name":"stderr","text":"Loss=0.050429195165634155 Batch_id=468 Accuracy=98.07: 100%|██████████| 469/469 [00:09<00:00, 47.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nTest set: Average loss: 0.0362, Accuracy: 9894/10000 (98.94%)\n\nEPOCH: 5\n","output_type":"stream"},{"name":"stderr","text":"Loss=0.01836872473359108 Batch_id=468 Accuracy=98.33: 100%|██████████| 469/469 [00:09<00:00, 49.78it/s] \n","output_type":"stream"},{"name":"stdout","text":"\nTest set: Average loss: 0.0335, Accuracy: 9893/10000 (98.93%)\n\nEPOCH: 6\n","output_type":"stream"},{"name":"stderr","text":"Loss=0.04676543176174164 Batch_id=468 Accuracy=98.50: 100%|██████████| 469/469 [00:09<00:00, 48.81it/s] \n","output_type":"stream"},{"name":"stdout","text":"\nTest set: Average loss: 0.0250, Accuracy: 9923/10000 (99.23%)\n\nEPOCH: 7\n","output_type":"stream"},{"name":"stderr","text":"Loss=0.01101953536272049 Batch_id=468 Accuracy=98.62: 100%|██████████| 469/469 [00:09<00:00, 47.28it/s] \n","output_type":"stream"},{"name":"stdout","text":"\nTest set: Average loss: 0.0253, Accuracy: 9923/10000 (99.23%)\n\nEPOCH: 8\n","output_type":"stream"},{"name":"stderr","text":"Loss=0.03975016623735428 Batch_id=468 Accuracy=98.67: 100%|██████████| 469/469 [00:09<00:00, 49.75it/s]  \n","output_type":"stream"},{"name":"stdout","text":"\nTest set: Average loss: 0.0248, Accuracy: 9925/10000 (99.25%)\n\nEPOCH: 9\n","output_type":"stream"},{"name":"stderr","text":"Loss=0.050169363617897034 Batch_id=468 Accuracy=98.65: 100%|██████████| 469/469 [00:09<00:00, 47.97it/s] \n","output_type":"stream"},{"name":"stdout","text":"\nTest set: Average loss: 0.0241, Accuracy: 9928/10000 (99.28%)\n\nEPOCH: 10\n","output_type":"stream"},{"name":"stderr","text":"Loss=0.05265502631664276 Batch_id=468 Accuracy=98.70: 100%|██████████| 469/469 [00:10<00:00, 44.90it/s]  \n","output_type":"stream"},{"name":"stdout","text":"\nTest set: Average loss: 0.0235, Accuracy: 9932/10000 (99.32%)\n\nEPOCH: 11\n","output_type":"stream"},{"name":"stderr","text":"Loss=0.011527098715305328 Batch_id=468 Accuracy=98.77: 100%|██████████| 469/469 [00:10<00:00, 46.63it/s] \n","output_type":"stream"},{"name":"stdout","text":"\nTest set: Average loss: 0.0239, Accuracy: 9929/10000 (99.29%)\n\nEPOCH: 12\n","output_type":"stream"},{"name":"stderr","text":"Loss=0.1252981275320053 Batch_id=468 Accuracy=98.73: 100%|██████████| 469/469 [00:09<00:00, 47.17it/s]  \n","output_type":"stream"},{"name":"stdout","text":"\nTest set: Average loss: 0.0251, Accuracy: 9921/10000 (99.21%)\n\nEPOCH: 13\n","output_type":"stream"},{"name":"stderr","text":"Loss=0.046946678310632706 Batch_id=468 Accuracy=98.77: 100%|██████████| 469/469 [00:10<00:00, 43.74it/s] \n","output_type":"stream"},{"name":"stdout","text":"\nTest set: Average loss: 0.0242, Accuracy: 9927/10000 (99.27%)\n\nEPOCH: 14\n","output_type":"stream"},{"name":"stderr","text":"Loss=0.061926040798425674 Batch_id=468 Accuracy=98.74: 100%|██████████| 469/469 [00:10<00:00, 46.75it/s] \n","output_type":"stream"},{"name":"stdout","text":"\nTest set: Average loss: 0.0250, Accuracy: 9926/10000 (99.26%)\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"id":"q-2xBkDS2mGa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}